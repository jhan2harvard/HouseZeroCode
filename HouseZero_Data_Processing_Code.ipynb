{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bf6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version:06.09.2023\n",
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This HouseZero_Data_Processing_Code.py file was generated on 2023-06-09 by Jung Min Han of CGBC\n",
    "\n",
    "# Modify the following file path for your local Drive\n",
    "FILEPATH = r\"C:\\Users\\ln\\Desktop\\HouseZeroData\"\n",
    "df_outliers = pd.read_csv(FILEPATH + \"Slab_temp_Year1.csv\")\n",
    "\n",
    "def calculate_covariance_matrix(data):\n",
    "    \"\"\"\n",
    "    Calculates the covariance matrix of the given data.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data. Each row represents a variable, and each column represents an observation.\n",
    "\n",
    "    Returns:\n",
    "        cov_matrix (ndarray): The covariance matrix of the data. The matrix is square with shape (N, N), where N is the number of variables.\n",
    "                              The element at position (i, j) represents the covariance between variable i and variable j.\n",
    "    \"\"\"\n",
    "    return np.cov(data, rowvar=False)\n",
    "\n",
    "def calculate_inverse_covariance_matrix(cov_matrix):\n",
    "    \"\"\"\n",
    "    Calculates the inverse covariance matrix.\n",
    "\n",
    "    Args:\n",
    "        cov_matrix (ndarray): The covariance matrix to calculate the inverse for. The matrix must be square and positive definite.\n",
    "\n",
    "    Returns:\n",
    "        inverse_cov_matrix (ndarray): The inverse of the covariance matrix. The matrix is square with the same shape as the input matrix.\n",
    "                                      The element at position (i, j) represents the inverse covariance between variable i and variable j.\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(cov_matrix)\n",
    "\n",
    "def calculate_mahalanobis_distance(x, mean, inv_cov):\n",
    "    \"\"\"\n",
    "    Calculates the Mahalanobis distance between a data point and a distribution with given mean and inverse covariance matrix.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): The data point for which to calculate the Mahalanobis distance.\n",
    "        mean (array-like): The mean of the distribution.\n",
    "        inv_cov (ndarray): The inverse covariance matrix of the distribution.\n",
    "\n",
    "    Returns:\n",
    "        distance (float): The Mahalanobis distance between the data point and the distribution.\n",
    "    \"\"\"\n",
    "    x_minus_mean = np.array(x - mean)\n",
    "    invalid = np.isnan(x_minus_mean) | np.isinf(x_minus_mean)\n",
    "    mean_value = np.nanmean(x_minus_mean)\n",
    "    x_minus_mean[invalid] = mean_value\n",
    "    return np.sqrt(np.dot(np.dot(x_minus_mean, inv_cov), x_minus_mean))\n",
    "\n",
    "def calculate_z_score(x, data):\n",
    "    \"\"\"\n",
    "    Calculates the Z-score of a data point given the data.\n",
    "\n",
    "    Args:\n",
    "        x (float): The data point for which to calculate the Z-score.\n",
    "        data (array-like): The data used to calculate the mean and standard deviation.\n",
    "\n",
    "    Returns:\n",
    "        z_score (float): The Z-score of the data point.\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_score = (x - mean) / std\n",
    "    return z_score\n",
    "\n",
    "def filter_outliers_z_score(data, threshold=3):\n",
    "    \"\"\"\n",
    "    Filters outliers from the data using Z-score.\n",
    "\n",
    "    Args:\n",
    "        data (array-like): The input data.\n",
    "        threshold (float): The threshold value to define outliers. Data points with absolute Z-scores greater than the threshold are considered outliers.\n",
    "\n",
    "    Returns:\n",
    "        filtered_data (array-like): The data with outliers filtered out.\n",
    "    \"\"\"\n",
    "    z_scores = calculate_z_score(data, data)\n",
    "    filtered_data = data[abs(z_scores) <= threshold]\n",
    "    return filtered_data\n",
    "\n",
    "def filter_outliers_diff(data, res=3):\n",
    "    \"\"\"\n",
    "    Filters outliers from the data based on the absolute difference.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data.\n",
    "        res (float): The threshold value to define outliers. Rows with absolute differences greater than res are considered outliers.\n",
    "\n",
    "    Returns:\n",
    "        filtered_data (pandas.DataFrame): The data with outliers filtered out and interpolated.\n",
    "    \"\"\"\n",
    "    # Filtering rows based on the absolute difference greater than res\n",
    "    diffIdx = data[abs(data.diff()) > res].index\n",
    "    # Dropping the filtered rows from the DataFrame\n",
    "    data = data.drop(diffIdx)\n",
    "    filtered_data = data.interpolate()\n",
    "    return filtered_data\n",
    "\n",
    "def filter_outliers_mahalanobis_distance(data, mean, inv_cov, threshold=3):\n",
    "    \"\"\"\n",
    "    Filters outliers from the data based on Mahalanobis distance.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data.\n",
    "        mean (array-like): The mean of the distribution.\n",
    "        inv_cov (ndarray): The inverse covariance matrix of the distribution.\n",
    "        threshold (float): The threshold value to define outliers. Data points with Mahalanobis distance greater than the threshold are considered outliers.\n",
    "\n",
    "    Returns:\n",
    "        filtered_data (pandas.DataFrame): The data with outliers filtered out.\n",
    "    \"\"\"\n",
    "    dist = [calculate_mahalanobis_distance(data.iloc[i], mean, inv_cov) for i in range(len(data))]\n",
    "    filtered_data = data[dist <= threshold]\n",
    "    return filtered_data\n",
    "\n",
    "def plot_slab_temp_with_outliers(data, zone_col):\n",
    "    \"\"\"\n",
    "    Plots the slab temperature data with outliers for a specific zone column.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data.\n",
    "        zone_col (str): The column representing the zone for which to plot the slab temperature.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    mean = np.mean(data[zone_col])\n",
    "    std = np.std(data[zone_col])\n",
    "\n",
    "    # Calculate the Mahalanobis Distance for each data point\n",
    "    dist = [calculate_mahalanobis_distance(data[zone_col].iloc[i], mean, inv_cov_matrix) for i in range(len(data))]\n",
    "    # Identify the outliers\n",
    "    outliers = np.where(dist > 3 * std)[0]\n",
    "    # Plot the slab temperature data\n",
    "    plt.plot(data.index, data[zone_col], 'o', markersize=2, label=\"Data\")\n",
    "\n",
    "    # Plot the outliers\n",
    "    if len(outliers) > 0:\n",
    "        plt.plot(data.iloc[outliers].index, data.iloc[outliers][zone_col], 'ro', markersize=4, label=\"Outliers\")\n",
    "\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Title\")\n",
    "    plt.title(zone_col + \" Slab Temperature\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def impute_missing_values_knn(data, k=3):\n",
    "    \"\"\"\n",
    "    Imputes missing values in the data using KNN imputation.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data with missing values.\n",
    "        k (int): The number of nearest neighbors to consider when imputing missing values.\n",
    "\n",
    "    Returns:\n",
    "        filled_data (pandas.DataFrame): The data with missing values imputed using KNN imputation.\n",
    "    \"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    filled_data = imputer.fit_transform(data)\n",
    "    filled_data = pd.DataFrame(filled_data, columns=data.columns)\n",
    "    return filled_data\n",
    "\n",
    "def impute_missing_values(data):\n",
    "    \"\"\"\n",
    "    Imputes missing values in the data using linear interpolation.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data with missing values.\n",
    "\n",
    "    Returns:\n",
    "        filled_data (pandas.DataFrame): The data with missing values imputed using linear interpolation.\n",
    "    \"\"\"\n",
    "    filled_data = data.interpolate()\n",
    "    return filled_data\n",
    "\n",
    "def impute_missing_values_rf(data, target_col):\n",
    "    \"\"\"\n",
    "    Imputes missing values in the data using Random Forest Regression with grid search.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input data with missing values.\n",
    "        target_col (str): The column representing the target variable with missing values.\n",
    "\n",
    "    Returns:\n",
    "        imputed_data (pandas.DataFrame): The data with missing values imputed using Random Forest Regression.\n",
    "    \"\"\"\n",
    "    # Split the data into two subsets: one with missing values and one without\n",
    "    missing_data = data[data[target_col].isnull()]\n",
    "    non_missing_data = data[~data[target_col].isnull()]\n",
    "\n",
    "    # Prepare the training data and target variable\n",
    "    X_train = non_missing_data.drop(target_col, axis=1)\n",
    "    y_train = non_missing_data[target_col]\n",
    "\n",
    "    # Define the parameter grid for grid search\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Create a Random Forest Regression model\n",
    "    rf_model = RandomForestRegressor()\n",
    "    # Create a GridSearchCV object with the model and parameter grid\n",
    "    grid_search = GridSearchCV(rf_model, param_grid=param_grid, cv=5)\n",
    "    # Fit the GridSearchCV on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    # Get the best model from the grid search\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    # Predict the missing values using the best model\n",
    "    missing_data[target_col] = best_rf_model.predict(missing_data.drop(target_col, axis=1))\n",
    "    # Combine the imputed data with the non-missing data\n",
    "    imputed_data = pd.concat([missing_data, non_missing_data])\n",
    "\n",
    "    return imputed_data\n",
    "\n",
    "    \n",
    "def main():\n",
    "    df = df_outliers  # Assuming df_outliers is defined and contains the necessary data\n",
    "\n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = calculate_covariance_matrix(df[[\"Z1_slab_temp\", \"Z2_slab_temp\"]])\n",
    "    # Calculate the inverse covariance matrix\n",
    "    inv_cov_matrix = calculate_inverse_covariance_matrix(cov_matrix)\n",
    "    # Plot slab temperature data with outliers for Z1_slab_temp\n",
    "    plot_slab_temp_with_outliers(df, \"Z1_slab_temp\")\n",
    "    # Plot slab temperature data with outliers for Z2_slab_temp\n",
    "    plot_slab_temp_with_outliers(df, \"Z2_slab_temp\")\n",
    "\n",
    "    # Example usage of Z-score outlier filtering\n",
    "    data = df[\"Z1_slab_temp\"]  # Example data\n",
    "    filtered_data = filter_outliers_z_score(data, threshold=3)\n",
    "    print(\"Filtered data:\", filtered_data)\n",
    "\n",
    "    # Impute missing values using Random Forest Regression with grid search\n",
    "    imputed_df = impute_missing_values_rf(df, \"Z1_slab_temp\")\n",
    "    print(\"Imputed data:\", imputed_df)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
